import torch
import gc
import os
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

# Fix for PyTorch 2.6+ compatibility with pyannote
# Must patch torch.load BEFORE importing whisperx/pyannote
_original_torch_load = torch.load
def _patched_torch_load(*args, **kwargs):
    kwargs['weights_only'] = False
    return _original_torch_load(*args, **kwargs)
torch.load = _patched_torch_load

import whisperx 
import time

# ===================== CONFIGURATION =====================
device = "cuda" 
# float16 = ‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤ int8 ‡∏ó‡∏±‡πâ‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß‡πÅ‡∏•‡∏∞‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏ö‡∏ô GPU
compute_type = "float16"
HF_TOKEN = os.environ.get("HF_TOKEN", "")

# Batch size - ‡∏¢‡∏¥‡πà‡∏á‡∏™‡∏π‡∏á‡∏¢‡∏¥‡πà‡∏á‡πÄ‡∏£‡πá‡∏ß ‡πÅ‡∏ï‡πà‡πÉ‡∏ä‡πâ VRAM ‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô
# A100 40GB: ‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ‡∏ñ‡∏∂‡∏á 32, RTX 3090: ‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ 16-24
batch_size = 24

# Language - ‡∏£‡∏∞‡∏ö‡∏∏‡∏†‡∏≤‡∏©‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ç‡πâ‡∏≤‡∏° language detection (‡πÄ‡∏£‡πá‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô 10-15%)
language = "th"

# ===================== TRANSCRIBE OPTIONS =====================
# Note: beam_size, best_of ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏™‡πà‡∏ï‡∏≠‡∏ô load_model ‡∏ú‡πà‡∏≤‡∏ô asr_options
transcribe_options = {
    "batch_size": batch_size,
    "language": language,
    "task": "transcribe",  # ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç! ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏õ‡πá‡∏ô "transcribe" ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà "translate" (‡πÅ‡∏õ‡∏•‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©)
}

# ===================== VAD OPTIONS =====================
# Voice Activity Detection - optimized for overlapping speech detection
vad_options = {
    "vad_onset": 0.400,      # Lower = more sensitive to speech start
    "vad_offset": 0.300,     # Lower = faster silence detection  
    "min_duration_on": 0.05, # Catch short speech segments (sec)
    "min_duration_off": 0.05, # Catch short pauses/interruptions (sec)
}

# ===================== SPEAKER DIARIZATION OPTIONS =====================
# Set expected speaker count for better overlapping speech detection
min_speakers = 2    # Minimum expected speakers (None = auto detect)
max_speakers = None # Maximum expected speakers (None = auto detect)

# ===================== MAIN SCRIPT =====================
# ‡∏£‡∏±‡∏ö path ‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏à‡∏≤‡∏Å user
audio_file = input("üìÅ ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÉ‡∏™‡πà path ‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏™‡∏µ‡∏¢‡∏á: ").strip().strip('"').strip("'")

# ‡πÇ‡∏´‡∏•‡∏î model ‡∏û‡∏£‡πâ‡∏≠‡∏° VAD options ‡∏ó‡∏µ‡πà optimize ‡πÅ‡∏•‡πâ‡∏ß
print("üîÑ Loading model...")
model_start = time.time()
model = whisperx.load_model(
    "large-v3",  # OpenAI Whisper large-v3 (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏´‡∏•‡∏≤‡∏¢‡∏†‡∏≤‡∏©‡∏≤‡∏£‡∏ß‡∏°‡∏ñ‡∏∂‡∏á‡πÑ‡∏ó‡∏¢) 
    device, 
    compute_type=compute_type,
    language=language,
    asr_options={
        "beam_size": 5,
        "best_of": 5,
        "patience": 1.5,
        # ‡πÄ‡∏õ‡∏¥‡∏î condition_on_previous_text ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ context ‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á ‡∏•‡∏î‡∏Å‡∏≤‡∏£ hallucinate
        "condition_on_previous_text": True,
        # ‡πÉ‡∏ä‡πâ‡∏´‡∏•‡∏≤‡∏¢ temperatures ‡πÄ‡∏õ‡πá‡∏ô fallback ‡∏ñ‡πâ‡∏≤ temp ‡∏ï‡πà‡∏≥‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ú‡∏•
        "temperatures": [0.0, 0.2, 0.4, 0.6, 0.8, 1.0],
        # ‡∏•‡∏î threshold ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö repetition ‡πÑ‡∏î‡πâ‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô
        "compression_ratio_threshold": 2.2,
        "log_prob_threshold": -0.8,
        "no_speech_threshold": 0.5,
        # initial_prompt ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏£‡∏π‡πâ‡∏ß‡πà‡∏≤‡∏ï‡πâ‡∏≠‡∏á transcribe ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢
        "initial_prompt": "‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡∏£‡∏±‡∏ö ‡∏ô‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡∏ñ‡∏≠‡∏î‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢",
        # ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏ã‡πâ‡∏≥
        "repetition_penalty": 1.1,
        "length_penalty": 1.0,
    },
    vad_options=vad_options,
)
model_time = time.time() - model_start
print(f"   ‚è±Ô∏è ‡πÇ‡∏´‡∏•‡∏î model: {model_time:.2f}s")

st_time = time.time()

# ‡πÇ‡∏´‡∏•‡∏î audio
print("üîÑ Loading audio...")
audio_start = time.time()
audio = whisperx.load_audio(audio_file)
audio_time = time.time() - audio_start
print(f"   ‚è±Ô∏è ‡πÇ‡∏´‡∏•‡∏î audio: {audio_time:.2f}s")

# Transcribe with optimized options
print("üéØ Transcribing...")
transcribe_start = time.time()
result = model.transcribe(audio, **transcribe_options)
transcribe_time = time.time() - transcribe_start
print(f"   ‚è±Ô∏è Transcribe: {transcribe_time:.2f}s")

# ‡∏•‡∏ö model ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏Ñ‡∏•‡∏µ‡∏¢‡∏£‡πå memory ‡∏Å‡πà‡∏≠‡∏ô diarization
del model
gc.collect()
torch.cuda.empty_cache()

# Speaker diarization
print("üë• Running speaker diarization...")
diarize_start = time.time()
diarize_model = whisperx.diarize.DiarizationPipeline(
    use_auth_token=HF_TOKEN, 
    device=device
)
diarize_segments = diarize_model(
    audio,
    min_speakers=min_speakers,
    max_speakers=max_speakers,
)
diarize_time = time.time() - diarize_start
print(f"   ‚è±Ô∏è Diarization: {diarize_time:.2f}s")

# Assign speakers to segments
result = whisperx.assign_word_speakers(diarize_segments, result)

# ‡∏•‡∏ö diarize model
del diarize_model
gc.collect()
torch.cuda.empty_cache()

total_time = time.time() - st_time

# ===================== OUTPUT =====================
print(f"\n‚è±Ô∏è Total processing time: {total_time:.2f} seconds")
print(f"   - ‡πÇ‡∏´‡∏•‡∏î audio: {audio_time:.2f}s")
print(f"   - Transcribe: {transcribe_time:.2f}s")
print(f"   - Diarization: {diarize_time:.2f}s")

# ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì audio length ‡πÅ‡∏•‡∏∞ speed
audio_length = len(audio) / 16000  # 16kHz sample rate
speed_factor = audio_length / total_time if total_time > 0 else 0
print(f"   - Audio length: {audio_length:.1f}s")
print(f"   - Speed: {speed_factor:.1f}x realtime")

print("\n" + "="*60)
print("üìä TRANSCRIPTION RESULTS")
print("="*60)
print(f"{'‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°':<10} {'‡πÄ‡∏ß‡∏•‡∏≤‡∏à‡∏ö':<10} {'‡∏Ñ‡∏ô‡∏û‡∏π‡∏î':<12} {'‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°'}")
print("-"*60)

def format_speaker(speaker):
    if speaker and speaker.startswith('SPEAKER_'):
        num = int(speaker.split('_')[1]) + 1
        return f"‡∏Ñ‡∏ô‡∏û‡∏π‡∏î {num}"
    return speaker or "Unknown"

def format_time(seconds):
    m = int(seconds // 60)
    s = int(seconds % 60)
    ms = int((seconds % 1) * 100)
    return f"{m:02d}:{s:02d}.{ms:02d}"

# ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏ï‡∏≤‡∏°‡πÄ‡∏ß‡∏•‡∏≤
segments = sorted(result['segments'], key=lambda x: x['start'])

for segment in segments:
    speaker = format_speaker(segment.get('speaker'))
    text = segment['text'].strip()
    start = format_time(segment['start'])
    end = format_time(segment['end'])
    print(f"{start:<10} {end:<10} {speaker:<12} {text}")

# Summary
print("\n" + "="*60)
print("üìà SPEAKER SUMMARY")
print("="*60)
speakers_time = {}
speakers_words = {}
for segment in segments:
    speaker = format_speaker(segment.get('speaker'))
    duration = segment['end'] - segment['start']
    word_count = len(segment['text'].split())
    speakers_time[speaker] = speakers_time.get(speaker, 0) + duration
    speakers_words[speaker] = speakers_words.get(speaker, 0) + word_count

total_speaking_time = sum(speakers_time.values())
for speaker, total in sorted(speakers_time.items()):
    percentage = (total / total_speaking_time * 100) if total_speaking_time > 0 else 0
    words = speakers_words.get(speaker, 0)
    print(f"  {speaker}: {format_time(total)} ({percentage:.1f}%) - {words} words")

# ‡∏£‡∏ß‡∏° text ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
combined_text = ' '.join(segment['text'].strip() for segment in segments)
print("\n" + "="*60)
print("üìù FULL TEXT:")
print("="*60)
print(combined_text)

# ‡πÅ‡∏™‡∏î‡∏á configuration ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ
print("\n" + "="*60)
print("‚öôÔ∏è CONFIGURATION USED:")
print("="*60)
print(f"  Model: large-v3")
print(f"  Compute type: {compute_type}")
print(f"  Batch size: {batch_size}")
print(f"  Beam size: 5")
print(f"  Language: {language}")